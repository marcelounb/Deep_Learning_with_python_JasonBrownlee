{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "22.1 Movie Review Sentiment Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO2xZRuJSIp4W3gB99lFeTs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcelounb/Deep_Learning_with_python_JasonBrownlee/blob/master/22_1_Movie_Review_Sentiment_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIYH9egwHnXT",
        "colab_type": "text"
      },
      "source": [
        "# 22.1 Movie Review Sentiment Classification\n",
        "The dataset used in this project is the Large Movie Review Dataset often referred to as theIMDB dataset. The Large Movie Review Dataset (often referred to as the IMDB dataset)contains 25,000 highly-polar movie reviews (good or bad) for training and the same amountagain for testing. The problem is to determine whether a given moving review has a positive ornegative sentiment.\n",
        "\n",
        "The data was collected by Stanford researchers and was used in a 2011 paper where a splitof 50-50 of the data was used for training and test. An accuracy of 88.89% was achieved.\n",
        "\n",
        "**22.2 Load the IMDB Dataset With Keras**\n",
        "\n",
        "Keras provides access to the IMDB dataset built-in3. The imdb.load_data() function allows you to load the dataset in a format that is ready for use in neural network and deep learning models. The words have been replaced by integers that indicate the absolute popularity of the word in the dataset. The sentences in each review are therefore comprised of a sequence of integers.\n",
        "Calling imdb.load_data() the first time will download the IMDB dataset to your computer and store it in your home directory under ~/.keras/datasets/imdb.pkl as a 32 megabyte file. \n",
        "\n",
        "Usefully, the imdb.load_data() function provides additional arguments including the number of top words to load (where words with a lower integer are marked as zero in the returned data), the number of top words to skip (to avoid the the’s) and the maximum length of reviews to support. Let’s load the dataset and calculate some properties of it. We will start o↵ by loading some libraries and loading the entire IMDB dataset as a training dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSrUz9qX5MQr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "97581d17-2edc-46c4-eb8e-3272ed92ec21"
      },
      "source": [
        "import numpy as np\n",
        "from keras.datasets import imdb\n",
        "from matplotlib import pyplot\n",
        "# load the dataset\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data()\n",
        "X = np.concatenate((X_train, X_test), axis=0)\n",
        "y = np.concatenate((y_train, y_test), axis=0)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOWhDmzTys9O",
        "colab_type": "text"
      },
      "source": [
        "Next we can display the shape of the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf0I1UIOxPlz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "6754788c-1480-4089-f560-9dfd0a058423"
      },
      "source": [
        "# summarize size\n",
        "print(\"Training data shape: \")\n",
        "print('X', X.shape)\n",
        "print('y', y.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data shape: \n",
            "X (50000,)\n",
            "y (50000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPwNs2q_zxfL",
        "colab_type": "text"
      },
      "source": [
        "We can also print the unique class values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7QH7KtayvU1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "439acfbe-e987-4d12-9035-cae738870d7a"
      },
      "source": [
        "  # Summarize number of classes\n",
        "print(\"Classes: \")\n",
        "print(np.unique(y))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classes: \n",
            "[0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYWaMDVG3hhg",
        "colab_type": "text"
      },
      "source": [
        "Next we can get an idea of the total number of unique words in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV81FLgaz0cW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "74b29211-921f-4d9b-efca-35f2792fe877"
      },
      "source": [
        "# Summarize number of words\n",
        "print(\"Number of words: \")\n",
        "print(len(np.unique(np.hstack(X))))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words: \n",
            "88585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4vTLrht4Brh",
        "colab_type": "text"
      },
      "source": [
        "Finally, we can get an idea of the average review length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ43GFL44CVF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "9485e0ac-1a57-4a65-da9b-9cc1e79d7b7c"
      },
      "source": [
        "# Summarize review length\n",
        "print(\"Review length: \")\n",
        "result = [len(x) for x in X]\n",
        "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
        "# plot review length\n",
        "pyplot.boxplot(result)\n",
        "pyplot.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review length: \n",
            "Mean 234.76 words (172.911495)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZjYtNrBMNu0",
        "colab_type": "text"
      },
      "source": [
        "We can see that the average review has just under 300 words with a standard deviation of just over 200 words.\n",
        "\n",
        "Looking a box and whisker plot for the review lengths in words, we can probably see an exponential distribution that we can probably cover the mass of the distribution with a clipped length of 400 to 500 words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvF4HbMjMfII",
        "colab_type": "text"
      },
      "source": [
        "# 22.3 Word Embeddings\n",
        "A recent breakthrough in the field of natural language processing is called word embedding.\n",
        "\n",
        "This is a technique where words are encoded as real-valued vectors in a high-dimensional space, where the similarity between words in terms of meaning translates to closeness in the vector space.\n",
        "\n",
        "Discrete words are mapped to vectors of continuous numbers. This is useful when working with natural language problems with neural networks and deep learning models are we require numbers as input.\n",
        "\n",
        "Keras provides a convenient way to convert positive integer representations of words into a word embedding by an Embedding layer.\n",
        "\n",
        "The layer takes arguments that define the mapping including the maximum number of expected words also called the vocabulary size (e.g. the largest integer value that will be seen as an integer). The layer also allows you to specify the dimensionality for each word vector, called the output dimension.\n",
        "\n",
        "We would like to use a word embedding representation for the IMDB dataset.\n",
        "\n",
        "Let’s say that we are only interested in the first 5,000 most used words in the dataset. Therefore our vocabulary size will be 5,000. We can choose to use a 32-dimension vector to represent each word. Finally, we may choose to cap the maximum review length at 500 words, truncating reviews longer than that and padding reviews shorter than that with 0 values.\n",
        "\n",
        "We would load the IMDB dataset as follows:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65Ssie3-4Jzs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "e8016e60-1243-4131-9e2e-36b2ccd4af02"
      },
      "source": [
        "imdb.load_data(num_words=5000)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]),\n",
              "         list([1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 4369, 2, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 2, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 2, 2, 349, 2637, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 2, 5, 2, 656, 245, 2350, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
              "         list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 2, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
              "         ...,\n",
              "         list([1, 11, 6, 230, 245, 2, 9, 6, 1225, 446, 2, 45, 2174, 84, 2, 4007, 21, 4, 912, 84, 2, 325, 725, 134, 2, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 2, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 2, 1209, 2295, 2, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 2, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 2, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 2, 5, 27, 710, 117, 2, 2, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 2, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 2, 2, 5, 4241, 18, 4, 2, 2, 250, 11, 1818, 2, 4, 4217, 2, 747, 1115, 372, 1890, 1006, 541, 2, 7, 4, 59, 2, 4, 3586, 2]),\n",
              "         list([1, 1446, 2, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 2, 40, 319, 2, 112, 2, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 2, 4, 2, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 2, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
              "         list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 2, 270, 2, 5, 2, 2, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 2, 9, 24, 6, 78, 1099, 17, 2345, 2, 21, 27, 2, 2, 5, 2, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 2, 2, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 2, 8, 2197, 2, 2, 544, 5, 383, 1271, 848, 1468, 2, 497, 2, 8, 1597, 2, 2, 21, 60, 27, 239, 9, 43, 2, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
              "        dtype=object), array([1, 0, 0, ..., 0, 1, 0])),\n",
              " (array([list([1, 591, 202, 14, 31, 6, 717, 10, 10, 2, 2, 5, 4, 360, 7, 4, 177, 2, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 2, 100, 28, 1668, 14, 31, 23, 27, 2, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 2, 38, 32, 25, 2, 451, 202, 14, 6, 717]),\n",
              "         list([1, 14, 22, 3443, 6, 176, 7, 2, 88, 12, 2679, 23, 1310, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 2, 2, 4, 2, 2801, 109, 1603, 21, 4, 22, 3861, 8, 6, 1193, 1330, 10, 10, 4, 105, 987, 35, 841, 2, 19, 861, 1074, 5, 1987, 2, 45, 55, 221, 15, 670, 2, 526, 14, 1069, 4, 405, 5, 2438, 7, 27, 85, 108, 131, 4, 2, 2, 3884, 405, 9, 3523, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 2, 2801, 45, 407, 31, 7, 41, 3778, 105, 21, 59, 299, 12, 38, 950, 5, 4521, 15, 45, 629, 488, 2733, 127, 6, 52, 292, 17, 4, 2, 185, 132, 1988, 2, 1799, 488, 2693, 47, 6, 392, 173, 4, 2, 4378, 270, 2352, 4, 1500, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2078, 7, 2, 861, 2, 5, 4182, 30, 3127, 2, 56, 4, 841, 5, 990, 692, 8, 4, 1669, 398, 229, 10, 10, 13, 2822, 670, 2, 14, 9, 31, 7, 27, 111, 108, 15, 2033, 19, 2, 1429, 875, 551, 14, 22, 9, 1193, 21, 45, 4829, 5, 45, 252, 8, 2, 6, 565, 921, 3639, 39, 4, 529, 48, 25, 181, 8, 67, 35, 1732, 22, 49, 238, 60, 135, 1162, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 2, 25, 203, 28, 8, 818, 12, 125, 4, 3077]),\n",
              "         list([1, 111, 748, 4368, 1133, 2, 2, 4, 87, 1551, 1262, 7, 31, 318, 2, 7, 4, 498, 2, 748, 63, 29, 2, 220, 686, 2, 5, 17, 12, 575, 220, 2507, 17, 6, 185, 132, 2, 16, 53, 928, 11, 2, 74, 4, 438, 21, 27, 2, 589, 8, 22, 107, 2, 2, 997, 1638, 8, 35, 2076, 2, 11, 22, 231, 54, 29, 1706, 29, 100, 2, 2425, 34, 2, 2, 2, 5, 2, 98, 31, 2122, 33, 6, 58, 14, 3808, 1638, 8, 4, 365, 7, 2789, 3761, 356, 346, 4, 2, 1060, 63, 29, 93, 11, 2, 11, 2, 33, 6, 58, 54, 1270, 431, 748, 7, 32, 2580, 16, 11, 94, 2, 10, 10, 4, 993, 2, 7, 4, 1766, 2634, 2164, 2, 8, 847, 8, 1450, 121, 31, 7, 27, 86, 2663, 2, 16, 6, 465, 993, 2006, 2, 573, 17, 2, 42, 4, 2, 37, 473, 6, 711, 6, 2, 7, 328, 212, 70, 30, 258, 11, 220, 32, 7, 108, 21, 133, 12, 9, 55, 465, 849, 3711, 53, 33, 2071, 1969, 37, 70, 1144, 4, 2, 1409, 74, 476, 37, 62, 91, 1329, 169, 4, 1330, 2, 146, 655, 2212, 5, 258, 12, 184, 2, 546, 5, 849, 2, 7, 4, 22, 1436, 18, 631, 1386, 797, 7, 4, 2, 71, 348, 425, 4320, 1061, 19, 2, 5, 2, 11, 661, 8, 339, 2, 4, 2455, 2, 7, 4, 1962, 10, 10, 263, 787, 9, 270, 11, 6, 2, 4, 2, 2, 121, 4, 2, 26, 4434, 19, 68, 1372, 5, 28, 446, 6, 318, 2, 8, 67, 51, 36, 70, 81, 8, 4392, 2294, 36, 1197, 8, 2, 2, 18, 6, 711, 4, 2, 26, 2, 1125, 11, 14, 636, 720, 12, 426, 28, 77, 776, 8, 97, 38, 111, 2, 2, 168, 1239, 2, 137, 2, 18, 27, 173, 9, 2399, 17, 6, 2, 428, 2, 232, 11, 4, 2, 37, 272, 40, 2708, 247, 30, 656, 6, 2, 54, 2, 3292, 98, 6, 2840, 40, 558, 37, 2, 98, 4, 2, 1197, 15, 14, 9, 57, 4893, 5, 4659, 6, 275, 711, 2, 2, 3292, 98, 6, 2, 10, 10, 2, 19, 14, 2, 267, 162, 711, 37, 2, 752, 98, 4, 2, 2378, 90, 19, 6, 2, 7, 2, 1810, 2, 4, 4770, 3183, 930, 8, 508, 90, 4, 1317, 8, 4, 2, 17, 2, 3965, 1853, 4, 1494, 8, 4468, 189, 4, 2, 2, 2, 4, 4770, 5, 95, 271, 23, 6, 2, 2, 2, 2, 33, 1526, 6, 425, 3155, 2, 4535, 1636, 7, 4, 4669, 2, 469, 4, 4552, 54, 4, 150, 2, 2, 280, 53, 2, 2, 18, 339, 29, 1978, 27, 2, 5, 2, 68, 1830, 19, 2, 2, 4, 1515, 7, 263, 65, 2132, 34, 6, 2, 2, 43, 159, 29, 9, 4706, 9, 387, 73, 195, 584, 10, 10, 1069, 4, 58, 810, 54, 14, 2, 117, 22, 16, 93, 5, 1069, 4, 192, 15, 12, 16, 93, 34, 6, 1766, 2, 33, 4, 2, 7, 15, 2, 2, 3286, 325, 12, 62, 30, 776, 8, 67, 14, 17, 6, 2, 44, 148, 687, 2, 203, 42, 203, 24, 28, 69, 2, 2, 11, 330, 54, 29, 93, 2, 21, 845, 2, 27, 1099, 7, 819, 4, 22, 1407, 17, 6, 2, 787, 7, 2460, 2, 2, 100, 30, 4, 3737, 3617, 3169, 2321, 42, 1898, 11, 4, 3814, 42, 101, 704, 7, 101, 999, 15, 1625, 94, 2926, 180, 5, 9, 2, 34, 2, 45, 6, 1429, 22, 60, 6, 1220, 31, 11, 94, 2, 96, 21, 94, 749, 9, 57, 975]),\n",
              "         ...,\n",
              "         list([1, 13, 1408, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 2, 21, 45, 184, 78, 4, 1492, 910, 769, 2290, 2515, 395, 4257, 5, 1454, 11, 119, 2, 89, 1036, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2280, 284, 1842, 2, 37, 315, 4, 226, 20, 272, 2942, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 2]),\n",
              "         list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 3684, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2960, 7, 743, 46, 1028, 9, 3531, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 2, 5, 9, 1861, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 1276, 92, 124, 51, 45, 1188, 71, 536, 13, 520, 14, 20, 6, 2302, 7, 470]),\n",
              "         list([1, 6, 52, 2, 430, 22, 9, 220, 2594, 8, 28, 2, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 2, 2, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 2, 9, 133, 1810, 11, 2, 2, 21, 4, 2, 2, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 2, 2, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 2, 2, 3406, 718, 2, 9, 6, 2, 17, 210, 5, 3281, 2, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 2, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 2, 19, 714, 727, 2, 382, 4, 91, 2, 439, 19, 14, 20, 9, 1441, 2, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643])],\n",
              "        dtype=object),\n",
              "  array([0, 1, 1, ..., 0, 0, 0])))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fli3wd21PDgz",
        "colab_type": "text"
      },
      "source": [
        "We would then use the Keras utility to truncate or pad the dataset to a length of 500 for each observation using the sequence.pad_sequences() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgHYVjg0P7Iz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6Jl6Bz0PCxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=500)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktQ7HXp4RGjY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e7e6acfe-32de-4688-975e-c1162bd04aac"
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     1,    14,    22,    16,    43,   530,\n",
              "         973,  1622,  1385,    65,   458,  4468,    66,  3941,     4,\n",
              "         173,    36,   256,     5,    25,   100,    43,   838,   112,\n",
              "          50,   670, 22665,     9,    35,   480,   284,     5,   150,\n",
              "           4,   172,   112,   167, 21631,   336,   385,    39,     4,\n",
              "         172,  4536,  1111,    17,   546,    38,    13,   447,     4,\n",
              "         192,    50,    16,     6,   147,  2025,    19,    14,    22,\n",
              "           4,  1920,  4613,   469,     4,    22,    71,    87,    12,\n",
              "          16,    43,   530,    38,    76,    15,    13,  1247,     4,\n",
              "          22,    17,   515,    17,    12,    16,   626,    18, 19193,\n",
              "           5,    62,   386,    12,     8,   316,     8,   106,     5,\n",
              "           4,  2223,  5244,    16,   480,    66,  3785,    33,     4,\n",
              "         130,    12,    16,    38,   619,     5,    25,   124,    51,\n",
              "          36,   135,    48,    25,  1415,    33,     6,    22,    12,\n",
              "         215,    28,    77,    52,     5,    14,   407,    16,    82,\n",
              "       10311,     8,     4,   107,   117,  5952,    15,   256,     4,\n",
              "       31050,     7,  3766,     5,   723,    36,    71,    43,   530,\n",
              "         476,    26,   400,   317,    46,     7,     4, 12118,  1029,\n",
              "          13,   104,    88,     4,   381,    15,   297,    98,    32,\n",
              "        2071,    56,    26,   141,     6,   194,  7486,    18,     4,\n",
              "         226,    22,    21,   134,   476,    26,   480,     5,   144,\n",
              "          30,  5535,    18,    51,    36,    28,   224,    92,    25,\n",
              "         104,     4,   226,    65,    16,    38,  1334,    88,    12,\n",
              "          16,   283,     5,    16,  4472,   113,   103,    32,    15,\n",
              "          16,  5345,    19,   178,    32], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWPGQ_mzRFxE",
        "colab_type": "text"
      },
      "source": [
        "Finally, later on, the first layer of our model would be an word embedding layer created using the Embedding class as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfdkuNBBSX7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Embedding"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHMf3G09Piqm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7176acb3-36f0-49b3-9678-495f70e010d4"
      },
      "source": [
        "Embedding(5000, 32, input_length=500)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.layers.embeddings.Embedding at 0x7f3514127a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rTiYZbuPAks",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1aaf754a-967d-4d71-9420-125e098406a0"
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     1,    14,    22,    16,    43,   530,\n",
              "         973,  1622,  1385,    65,   458,  4468,    66,  3941,     4,\n",
              "         173,    36,   256,     5,    25,   100,    43,   838,   112,\n",
              "          50,   670, 22665,     9,    35,   480,   284,     5,   150,\n",
              "           4,   172,   112,   167, 21631,   336,   385,    39,     4,\n",
              "         172,  4536,  1111,    17,   546,    38,    13,   447,     4,\n",
              "         192,    50,    16,     6,   147,  2025,    19,    14,    22,\n",
              "           4,  1920,  4613,   469,     4,    22,    71,    87,    12,\n",
              "          16,    43,   530,    38,    76,    15,    13,  1247,     4,\n",
              "          22,    17,   515,    17,    12,    16,   626,    18, 19193,\n",
              "           5,    62,   386,    12,     8,   316,     8,   106,     5,\n",
              "           4,  2223,  5244,    16,   480,    66,  3785,    33,     4,\n",
              "         130,    12,    16,    38,   619,     5,    25,   124,    51,\n",
              "          36,   135,    48,    25,  1415,    33,     6,    22,    12,\n",
              "         215,    28,    77,    52,     5,    14,   407,    16,    82,\n",
              "       10311,     8,     4,   107,   117,  5952,    15,   256,     4,\n",
              "       31050,     7,  3766,     5,   723,    36,    71,    43,   530,\n",
              "         476,    26,   400,   317,    46,     7,     4, 12118,  1029,\n",
              "          13,   104,    88,     4,   381,    15,   297,    98,    32,\n",
              "        2071,    56,    26,   141,     6,   194,  7486,    18,     4,\n",
              "         226,    22,    21,   134,   476,    26,   480,     5,   144,\n",
              "          30,  5535,    18,    51,    36,    28,   224,    92,    25,\n",
              "         104,     4,   226,    65,    16,    38,  1334,    88,    12,\n",
              "          16,   283,     5,    16,  4472,   113,   103,    32,    15,\n",
              "          16,  5345,    19,   178,    32], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4hG1lIPSm4F",
        "colab_type": "text"
      },
      "source": [
        "The output of this first layer would be a matrix with the size 32×500 for a given review training or test pattern in integer format.\n",
        "\n",
        "Now that we know how to load the IMDB dataset in Keras and how to use a word embedding representation for it, let’s develop and evaluate some models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxzjZRIBSftI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}